{"cells":[{"source":"# Code-along 2023-12-12 Retrieval Augmented Generation\n\n## Conceptual Overview of RAG\nAlso available at [https://whimsical.com/rag-diagram-Ed1YXhRAVd19rLiuk15FWG]\n![Screenshot 2023-12-04 at 3.43.11 PM](images/Screenshot%202023-12-04%20at%203.43.11%20PM.png)","metadata":{},"cell_type":"markdown","id":"19fadf5b-22d1-4672-aaf7-d831bf790a30"},{"source":"!pip install -U newsapi-python llama-index huggingface_hub[inference]","metadata":{"executionCancelledAt":null,"executionTime":11158,"lastExecutedAt":1702395602715,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"!pip install -U newsapi-python llama-index huggingface_hub[inference]","outputsMetadata":{"0":{"height":589,"type":"stream"}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false}},"cell_type":"code","id":"7d64301c-7a6d-4c45-be83-54fa5db1183c","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":"Defaulting to user installation because normal site-packages is not writeable\nCollecting newsapi-python\n  Downloading newsapi_python-0.2.7-py2.py3-none-any.whl (7.9 kB)\nCollecting llama-index\n  Downloading llama_index-0.9.14.post2-py3-none-any.whl.metadata (8.2 kB)\nRequirement already satisfied: huggingface_hub[inference] in /usr/local/lib/python3.8/dist-packages (0.11.0)\nCollecting huggingface_hub[inference]\n  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.8/dist-packages (from newsapi-python) (2.31.0)\nCollecting SQLAlchemy>=1.4.49 (from SQLAlchemy[asyncio]>=1.4.49->llama-index)\n  Downloading SQLAlchemy-2.0.23-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\nCollecting aiohttp<4.0.0,>=3.8.6 (from llama-index)\n  Downloading aiohttp-3.9.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\nCollecting beautifulsoup4<5.0.0,>=4.12.2 (from llama-index)\n  Downloading beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.0/143.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting dataclasses-json (from llama-index)\n  Downloading dataclasses_json-0.6.3-py3-none-any.whl.metadata (25 kB)\nRequirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.8/dist-packages (from llama-index) (1.2.13)\nCollecting fsspec>=2023.5.0 (from llama-index)\n  Downloading fsspec-2023.12.2-py3-none-any.whl.metadata (6.8 kB)\nCollecting httpx (from llama-index)\n  Downloading httpx-0.25.2-py3-none-any.whl.metadata (6.9 kB)\nCollecting nest-asyncio<2.0.0,>=1.5.8 (from llama-index)\n  Downloading nest_asyncio-1.5.8-py3-none-any.whl.metadata (2.8 kB)\nCollecting nltk<4.0.0,>=3.8.1 (from llama-index)\n  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from llama-index) (1.23.2)\nCollecting openai>=1.1.0 (from llama-index)\n  Downloading openai-1.3.8-py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from llama-index) (1.5.1)\nCollecting tenacity<9.0.0,>=8.2.0 (from llama-index)\n  Downloading tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\nCollecting tiktoken>=0.3.3 (from llama-index)\n  Downloading tiktoken-0.5.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.8/dist-packages (from llama-index) (4.5.0)\nRequirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from llama-index) (0.8.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface_hub[inference]) (3.8.0)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.8/dist-packages (from huggingface_hub[inference]) (4.64.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface_hub[inference]) (6.0.1)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface_hub[inference]) (21.3)\nRequirement already satisfied: pydantic<2.0,>1.1 in /usr/local/lib/python3.8/dist-packages (from huggingface_hub[inference]) (1.10.12)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (21.4.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (6.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (1.8.1)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (1.3.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (1.2.0)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (4.0.2)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.8/dist-packages (from beautifulsoup4<5.0.0,>=4.12.2->llama-index) (2.3.2.post1)\nRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.8/dist-packages (from deprecated>=1.2.9.3->llama-index) (1.14.1)\nRequirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (8.1.3)\nRequirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (1.3.2)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (2022.8.17)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.8/dist-packages (from openai>=1.1.0->llama-index) (3.6.1)\nCollecting distro<2,>=1.7.0 (from openai>=1.1.0->llama-index)\n  Downloading distro-1.8.0-py3-none-any.whl (20 kB)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.8/dist-packages (from openai>=1.1.0->llama-index) (1.2.0)\nRequirement already satisfied: certifi in /usr/lib/python3/dist-packages (from httpx->llama-index) (2019.11.28)\nCollecting httpcore==1.* (from httpx->llama-index)\n  Downloading httpcore-1.0.2-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: idna in /usr/lib/python3/dist-packages (from httpx->llama-index) (2.8)\nCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index)\n  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.9->huggingface_hub[inference]) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0->newsapi-python) (2.0.12)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3.0.0->newsapi-python) (1.25.8)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.8/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index) (1.1.3)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from typing-inspect>=0.8.0->llama-index) (0.4.3)\nCollecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index)\n  Downloading marshmallow-3.20.1-py3-none-any.whl.metadata (7.8 kB)\nRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas->llama-index) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->llama-index) (2022.7)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->llama-index) (1.14.0)\nDownloading llama_index-0.9.14.post2-py3-none-any.whl (943 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m943.5/943.5 kB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading aiohttp-3.9.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2023.12.2-py3-none-any.whl (168 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.0/169.0 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nest_asyncio-1.5.8-py3-none-any.whl (5.3 kB)\nDownloading openai-1.3.8-py3-none-any.whl (221 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.5/221.5 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httpx-0.25.2-py3-none-any.whl (74 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading SQLAlchemy-2.0.23-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m110.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tenacity-8.2.3-py3-none-any.whl (24 kB)\nDownloading tiktoken-0.5.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m117.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\nDownloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.7/311.7 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: tenacity, SQLAlchemy, nltk, nest-asyncio, h11, fsspec, distro, beautifulsoup4, tiktoken, newsapi-python, marshmallow, huggingface_hub, httpcore, aiohttp, httpx, dataclasses-json, openai, llama-index\n\u001b[33m  WARNING: The script nltk is installed in '/home/repl/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m  WARNING: The script distro is installed in '/home/repl/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m  WARNING: The script huggingface-cli is installed in '/home/repl/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m  WARNING: The script httpx is installed in '/home/repl/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m  WARNING: The script openai is installed in '/home/repl/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m  WARNING: The script llamaindex-cli is installed in '/home/repl/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngremlinpython 3.6.1 requires aiohttp<=3.8.1,>=3.8.0, but you have aiohttp 3.9.1 which is incompatible.\nsqlalchemy-redshift 0.8.11 requires SQLAlchemy<2.0.0,>=0.9.2, but you have sqlalchemy 2.0.23 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed SQLAlchemy-2.0.23 aiohttp-3.9.1 beautifulsoup4-4.12.2 dataclasses-json-0.6.3 distro-1.8.0 fsspec-2023.12.2 h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 huggingface_hub-0.19.4 llama-index-0.9.14.post2 marshmallow-3.20.1 nest-asyncio-1.5.8 newsapi-python-0.2.7 nltk-3.8.1 openai-1.3.8 tenacity-8.2.3 tiktoken-0.5.2\n"}]},{"source":"## Get Data To Test With\n\nWe use will the [News API](https://newsapi.org/). You can get a free developer account to retrieve small amounts of information like we will use for this project.\n\nTo start, we will build a RAG system based on November 2023 stories about the open source Llama2 model. But you can change the parameters below and re-run this code to build a knowledge base on other topics or from other timelines.","metadata":{},"cell_type":"markdown","id":"bb35400b-7063-4024-ab4d-8cf6768e0aa4"},{"source":"import json\nfrom newsapi import NewsApiClient\nimport os\n\nnews_api_key = os.getenv('NEWS_API_KEY')\nnewsapi = NewsApiClient(api_key=news_api_key)\nget_articles_now = False\narticles_path = './saved_articles.json'\n\nif get_articles_now:\n    try:\n        all_articles = newsapi.get_everything(q='Llama2',\n                                              from_param='2023-11-01',\n                                              to='2023-11-30',\n                                              language='en',\n                                              sort_by='publishedAt',\n                                              page_size=100)\n        with open(articles_path, 'w') as f:\n            json.dump(all_articles, f)\n    except Exception as e:\n        print(\"An error occurred:\", str(e))\nelse:\n    with open(articles_path, 'r') as f:\n        all_articles = json.load(f)\n\nprint(f'Retrieved {len(all_articles[\"articles\"])} articles')","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1702395602768,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import json\nfrom newsapi import NewsApiClient\nimport os\n\nnews_api_key = os.getenv('NEWS_API_KEY')\nnewsapi = NewsApiClient(api_key=news_api_key)\nget_articles_now = False\narticles_path = './saved_articles.json'\n\nif get_articles_now:\n    try:\n        all_articles = newsapi.get_everything(q='Llama2',\n                                              from_param='2023-11-01',\n                                              to='2023-11-30',\n                                              language='en',\n                                              sort_by='publishedAt',\n                                              page_size=100)\n        with open(articles_path, 'w') as f:\n            json.dump(all_articles, f)\n    except Exception as e:\n        print(\"An error occurred:\", str(e))\nelse:\n    with open(articles_path, 'r') as f:\n        all_articles = json.load(f)\n\nprint(f'Retrieved {len(all_articles[\"articles\"])} articles')","outputsMetadata":{"0":{"height":37,"type":"stream"}}},"cell_type":"code","id":"1918f96a-a172-43a2-9d40-bd41885ce089","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":"Retrieved 100 articles\n"}]},{"source":"Inspect a single item in `all_articles['articles']` to see the format","metadata":{},"cell_type":"markdown","id":"98e53486-60cd-4d07-b122-5f212a1da4ed"},{"source":"all_articles['articles'][1]","metadata":{"executionCancelledAt":null,"executionTime":56,"lastExecutedAt":1702395602824,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"all_articles['articles'][1]"},"cell_type":"code","id":"0cd6dca7-919d-4a9f-b1b2-01bb9ea1d81c","execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":"{'source': {'id': None, 'name': 'Biztoc.com'},\n 'author': 'medium.datadriveninvestor.com',\n 'title': 'How to choose between ChatGPT and Open Source LLMs in Finance',\n 'description': 'How to choose between ChatGPT and Open Source LLMs in Finance Many consulting companies present LLMs and GenAI products to CEOs, CFOs, COOs, and CTOs. While these products may seem appealing, companies should remember the distinction between using ChatGPT wit…',\n 'url': 'https://biztoc.com/x/71d1b51b76f35b0e',\n 'urlToImage': 'https://c.biztoc.com/p/71d1b51b76f35b0e/s.webp',\n 'publishedAt': '2023-12-03T10:52:23Z',\n 'content': 'How to choose between ChatGPT and Open Source LLMs in FinanceMany consulting companies present LLMs and GenAI products to CEOs, CFOs, COOs, and CTOs. While these products may seem appealing, companie… [+227 chars]'}"},"metadata":{},"execution_count":3}]},{"source":"It's not ideal that the articles are truncated. That's because a limitation of using a free account. But we will still be able to work with this.\n\nNow look at a sample of article titles for an overview of our content.","metadata":{},"cell_type":"markdown","id":"8ce8481a-76cc-4973-a348-52ad0f8dbaf6"},{"source":"titles = [i['title'] for i in all_articles['articles']]\ntitles[:20]","metadata":{"executionCancelledAt":null,"executionTime":51,"lastExecutedAt":1702395602875,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"titles = [i['title'] for i in all_articles['articles']]\ntitles[:20]"},"cell_type":"code","id":"1bbb4705-9cc8-4a97-af74-98a59453ad01","execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":"['Researchers scanned public repos and found 1,681 exposed Hugging Face API tokens belonging to Meta, Microsoft, Google, and others, many with write permissions',\n 'How to choose between ChatGPT and Open Source LLMs in Finance',\n \"ChatGPT was treated like the 'second coming of the messiah' and its impact was a big surprise, says Meta's AI chief\",\n 'Show HN: AI That Studies for You',\n \"ChatGPT was treated like the 'second coming of the messiah' and its impact was a big surprise, says Meta's AI chief\",\n 'Ambarella : Q3 FY2024 Earnings Call Transcript',\n 'Package and deploy classical ML and LLMs easily with Amazon SageMaker, part 1: PySDK Improvements',\n 'Alibaba releases 72B LLM with 32k context length',\n 'Revolutionizing Business Solutions with SAP BTP: A New Era of LLM Agnosticism',\n 'Use Ollama LLM Models Locally with Laravel',\n 'Operationalize LLM Evaluation at Scale using Amazon SageMaker Clarify and MLOps services',\n 'AWS unveils new tools and services for ‘supernova’ of generative AI',\n 'Perplexity AI unveils ‘online’ LLMs that could dethrone Google Search',\n \"Perplexity announces two online LLMs that can access the internet to deliver up-to-date responses, available via Perplexity's API and Labs web interface\",\n 'The reason behind Sam Altman’s ouster leaves OpenAI staff uneasy after his return',\n 'Perplexity Introduces Online LLMs With Real-Time Information via @sejournal, @kristileilani',\n 'McAfee’s CTO on AI and the ‘cat-and-mouse’ game with holiday scams',\n 'Amazon’s Selipsky Pounces On Safe, Diverse AI',\n \"We tried Brave's AI chatbot Leo: It talks a lot about privacy, but is it truly private?\",\n 'Research To Transform India’s Consumer Grievance Redressal System Through GenAI']"},"metadata":{},"execution_count":4}]},{"source":"Make a list with `content` from each article that we can use to populate vector DB","metadata":{},"cell_type":"markdown","id":"5018831d-5538-4b08-89c7-5f71d62ad3be"},{"source":"articles_text = [i['content'] for i in all_articles['articles']]","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1702395602923,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"articles_text = [i['content'] for i in all_articles['articles']]"},"cell_type":"code","id":"77478820-eb3a-4435-8c41-b9f3a9c30aed","execution_count":5,"outputs":[]},{"source":"## Build Basic RAG system","metadata":{},"id":"92a9caca-70fd-4ac0-aa15-1bee55c456d3","cell_type":"markdown"},{"source":"from llama_index import VectorStoreIndex, ServiceContext, Document\nfrom openai import OpenAI\n\n# Make client to access the model\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n# Specify documents to be retrieved\ndocuments = [Document(text=t) for t in articles_text]\n# Create the vector store that we use to find relevant documents\nindex = VectorStoreIndex.from_documents(documents)\n# A query engine is our final goal. The thing we can query\nquery_engine = index.as_query_engine(similarity_top_k=2)","metadata":{"executionCancelledAt":null,"executionTime":7718,"lastExecutedAt":1702395610642,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from llama_index import VectorStoreIndex, ServiceContext, Document\nfrom openai import OpenAI\n\n# Make client to access the model\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n# Specify documents to be retrieved\ndocuments = [Document(text=t) for t in articles_text]\n# Create the vector store that we use to find relevant documents\nindex = VectorStoreIndex.from_documents(documents)\n# A query engine is our final goal. The thing we can query\nquery_engine = index.as_query_engine(similarity_top_k=2)","collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"outputsMetadata":{"0":{"height":57,"type":"stream"}}},"id":"f81d6d0e-986b-49f4-94e1-7315a7f0bd67","cell_type":"code","execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":"[nltk_data] Downloading package punkt to /tmp/llama_index...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n"}]},{"source":"### Test Our Query Engine","metadata":{},"cell_type":"markdown","id":"d1f995e8-d098-4350-8e03-973cc5b67b45"},{"source":"Test a query with `query_engine.query`","metadata":{},"cell_type":"markdown","id":"27a55d5b-2d86-421e-ac82-24e53a9e53fb"},{"source":"query_engine.query(\"What AWS service can be used to deploy Llama2 models?\")","metadata":{"executionCancelledAt":null,"executionTime":731,"lastExecutedAt":1702395611374,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"query_engine.query(\"What AWS service can be used to deploy Llama2 models?\")"},"cell_type":"code","id":"08570d8e-ba04-4042-90ca-f800d82d3732","execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":"Response(response='Amazon Bedrock', source_nodes=[NodeWithScore(node=TextNode(id_='a14a86b3-6120-454a-97e1-ad82daf9ac86', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='626e19fd-6e9a-433b-aa4e-eff7d252fe96', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='49e4eba93e87faf4d534de4afc098521ee66886bfad74addecc907441b82e8a9'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='5b082e66-75c9-443e-bb06-8f93651b6ebc', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='22842fb263360082eb2f867ff62cc4cd19fadbfc30797f7c71f78e06ee2abd5c'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='5811e5a8-5ace-46cc-9b31-3759d210d711', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='b2396cf0076d8401589f67edc7fa8eb370ec4bc1941dff832c0af8f680f26f84')}, hash='49e4eba93e87faf4d534de4afc098521ee66886bfad74addecc907441b82e8a9', text='Today, were announcing the availability of Metas Llama 2 Chat 13B large language model (LLM) on Amazon Bedrock. With this launch, Amazon Bedrock becomes the first public cloud service to offer a full… [+8111 chars]', start_char_idx=0, end_char_idx=214, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8376725235589151), NodeWithScore(node=TextNode(id_='470558e0-8de6-4c04-8e77-50358cd73437', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='519d4f32-72e3-4312-9397-9027dd65095c', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='7c2ad39575abacfecca507988b6e7981e2a39b08978f9b1813d6c4fcf97022d8'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='6a3dbc2f-854d-4e82-a776-dd9c3d6de0bd', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='0ca5378ef6ef296dacc86a2b3fd9b1f0d317d553da913870daccff6f77601178'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='2f575df0-507f-488a-8e57-c94e667102d1', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='762721457336c4c0760abd7b25a419d888e64e22287fab88f9c1be1188a3296a')}, hash='7c2ad39575abacfecca507988b6e7981e2a39b08978f9b1813d6c4fcf97022d8', text='Run Large-Language Models (LLMs) directly in your browser ! \\r\\nLearn More: API Reference\\r\\nDeveloped By: RDS \\r\\nThis web demo enables you to run LLM models from Hugging Face (GGUF/GGML/tiny-llama2/starc… [+34 chars]', start_char_idx=0, end_char_idx=212, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8176330734196251)], metadata={'a14a86b3-6120-454a-97e1-ad82daf9ac86': {}, '470558e0-8de6-4c04-8e77-50358cd73437': {}})"},"metadata":{},"execution_count":7}]},{"source":"Create a convenience function to return only the most relevant info (the response and the text that informed that response.)","metadata":{},"cell_type":"markdown","id":"cbdf93a0-1f67-4b2b-97af-e7539be83efd"},{"source":"def search(query):\n    response = query_engine.query(query)\n    output = {'response': response.response,\n              'retrieved_nodes': [p.text for p in response.source_nodes]\n    }\n    return output","metadata":{"executionCancelledAt":null,"executionTime":53,"lastExecutedAt":1702395611427,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"def search(query):\n    response = query_engine.query(query)\n    output = {'response': response.response,\n              'retrieved_nodes': [p.text for p in response.source_nodes]\n    }\n    return output"},"cell_type":"code","id":"523526b6-1800-4bf7-bd81-0110d1fa5dc3","execution_count":8,"outputs":[]},{"source":"Test our new function","metadata":{},"cell_type":"markdown","id":"14c294ad-3052-49e6-b1a8-19f86bdceca1"},{"source":"search(\"What AWS service an be used to deploy Llama2 models?\")","metadata":{"executionCancelledAt":null,"executionTime":648,"lastExecutedAt":1702395612076,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"search(\"What AWS service an be used to deploy Llama2 models?\")"},"cell_type":"code","id":"e688c87a-7806-4654-8950-829b942ac2a0","execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":"{'response': 'Amazon Bedrock',\n 'retrieved_nodes': ['Today, were announcing the availability of Metas Llama 2 Chat 13B large language model (LLM) on Amazon Bedrock. With this launch, Amazon Bedrock becomes the first public cloud service to offer a full… [+8111 chars]',\n  'Run Large-Language Models (LLMs) directly in your browser ! \\r\\nLearn More: API Reference\\r\\nDeveloped By: RDS \\r\\nThis web demo enables you to run LLM models from Hugging Face (GGUF/GGML/tiny-llama2/starc… [+34 chars]']}"},"metadata":{},"execution_count":9}]},{"source":"### Use Llama2 instead of OpenAI models","metadata":{},"cell_type":"markdown","id":"d2d3c127-9021-4441-a285-b226cf1acf10"},{"source":"from llama_index.llms import HuggingFaceInferenceAPI\nfrom llama_index import ServiceContext, VectorStoreIndex\n\nmodel_name = 'HuggingFaceH4/zephyr-7b-beta'\n\n# Build service_context here that uses model served on HF endpoint\nhf_model = HuggingFaceInferenceAPI(model_name=model_name)\n\n# or if you have an HF token, use\n# HF_TOKEN = os.getenv('HF_TOKEN')\n# remotely_run = HuggingFaceInferenceAPI(\n#     model_name=model_name, token=HF_TOKEN\n# )","metadata":{"executionCancelledAt":null,"executionTime":120,"lastExecutedAt":1702395612196,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from llama_index.llms import HuggingFaceInferenceAPI\nfrom llama_index import ServiceContext, VectorStoreIndex\n\nmodel_name = 'HuggingFaceH4/zephyr-7b-beta'\n\n# Build service_context here that uses model served on HF endpoint\nhf_model = HuggingFaceInferenceAPI(model_name=model_name)\n\n# or if you have an HF token, use\n# HF_TOKEN = os.getenv('HF_TOKEN')\n# remotely_run = HuggingFaceInferenceAPI(\n#     model_name=model_name, token=HF_TOKEN\n# )"},"cell_type":"code","id":"dae13ecb-4e39-470d-a44c-2948dc959133","execution_count":10,"outputs":[]},{"source":"# Create a service_context object with llm=hf_model.\n# The service_context gives a lot of the general configuration parameters\nservice_context = ServiceContext.from_defaults(llm=hf_model)\n\n# Create VectorStoreIndex using same command we used before, but pass in the service_context=service_context.\nindex = VectorStoreIndex.from_documents(documents, service_context=service_context)\n\n# Create the query engine\nquery_engine = index.as_query_engine(similarity_top_k=2)","metadata":{"executionCancelledAt":null,"executionTime":3450,"lastExecutedAt":1702395615647,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Create a service_context object with llm=hf_model.\n# The service_context gives a lot of the general configuration parameters\nservice_context = ServiceContext.from_defaults(llm=hf_model)\n\n# Create VectorStoreIndex using same command we used before, but pass in the service_context=service_context.\nindex = VectorStoreIndex.from_documents(documents, service_context=service_context)\n\n# Create the query engine\nquery_engine = index.as_query_engine(similarity_top_k=2)"},"cell_type":"code","id":"dff45327-c21e-46ab-8a6e-e6a798e5d04c","execution_count":11,"outputs":[]},{"source":"Test it","metadata":{},"cell_type":"markdown","id":"c7add35f-f12e-4945-a4cb-5dda61bf5b06"},{"source":"query_engine.query(\"What AWS service can be used to deploy Llama2 models?\")","metadata":{"executionCancelledAt":null,"executionTime":200,"lastExecutedAt":1702395615849,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"query_engine.query(\"What AWS service can be used to deploy Llama2 models?\")"},"cell_type":"code","id":"7b66f1f5-ebf8-47e4-93b6-7a7a78ee488e","execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":"Response(response='\\n\\nThe AWS service that can be used to deploy Llama2 models is Amazon Bedrock. This was announced in a recent announcement by Meta, making Amazon Bedrock the first public cloud service to offer a full-featured deployment option for Llama2 models.', source_nodes=[NodeWithScore(node=TextNode(id_='190f06ab-70bb-4fba-9551-ff47d0b02a71', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='626e19fd-6e9a-433b-aa4e-eff7d252fe96', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='49e4eba93e87faf4d534de4afc098521ee66886bfad74addecc907441b82e8a9'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='e107961d-9c3f-40af-b73d-be325c36af63', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='22842fb263360082eb2f867ff62cc4cd19fadbfc30797f7c71f78e06ee2abd5c'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='8d9a620a-202f-4c11-bf1d-98ad0dc12086', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='b2396cf0076d8401589f67edc7fa8eb370ec4bc1941dff832c0af8f680f26f84')}, hash='49e4eba93e87faf4d534de4afc098521ee66886bfad74addecc907441b82e8a9', text='Today, were announcing the availability of Metas Llama 2 Chat 13B large language model (LLM) on Amazon Bedrock. With this launch, Amazon Bedrock becomes the first public cloud service to offer a full… [+8111 chars]', start_char_idx=0, end_char_idx=214, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8375443957360625), NodeWithScore(node=TextNode(id_='5fc63d79-3ed2-476e-b429-7294d727c22a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='519d4f32-72e3-4312-9397-9027dd65095c', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='7c2ad39575abacfecca507988b6e7981e2a39b08978f9b1813d6c4fcf97022d8'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='2b974cf1-f8cd-489b-aa71-a07cadef906a', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='0ca5378ef6ef296dacc86a2b3fd9b1f0d317d553da913870daccff6f77601178'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='2df8724d-68cc-42f6-8888-3e0f0f9569f4', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='762721457336c4c0760abd7b25a419d888e64e22287fab88f9c1be1188a3296a')}, hash='7c2ad39575abacfecca507988b6e7981e2a39b08978f9b1813d6c4fcf97022d8', text='Run Large-Language Models (LLMs) directly in your browser ! \\r\\nLearn More: API Reference\\r\\nDeveloped By: RDS \\r\\nThis web demo enables you to run LLM models from Hugging Face (GGUF/GGML/tiny-llama2/starc… [+34 chars]', start_char_idx=0, end_char_idx=212, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8176330734196251)], metadata={'190f06ab-70bb-4fba-9551-ff47d0b02a71': {}, '5fc63d79-3ed2-476e-b429-7294d727c22a': {}})"},"metadata":{},"execution_count":12}]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}